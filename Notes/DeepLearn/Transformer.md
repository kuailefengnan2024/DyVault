
### Transformer 构建 LLM 流程
**输入句子**: “这是训练语料”  

---

#### 1. 分词与嵌入层 (Tokenization & Embedding)
**操作**: 将句子分词并映射为词嵌入向量。  
- 假设词汇表大小为 1000，嵌入维度为 512。  
- 分词结果: ["这是", "训练", "语料"]（简化为词级别分词）。  
- 输入维度: [3]（3 个词）。  
- 输出维度: [3, 512]（每个词映射为 512 维嵌入向量）。  
```
  输入: ["这是", "训练", "语料"]
  输出: tensor([
    [0.12, -0.34, 0.56, ..., 0.78],  # "这是" 的嵌入
    [0.45, 0.67, -0.23, ..., 0.19],  # "训练" 的嵌入
    [0.89, -0.12, 0.33, ..., -0.45]  # "语料" 的嵌入
  ])（形状: [3, 512]，假设的词嵌入值）
```

---

#### 2. 位置编码 (Positional Encoding)
**操作**: 为每个词添加位置信息，使用正弦和余弦函数生成。  
- 输入维度: [3, 512]。  
- 输出维度: [3, 512]（词嵌入 + 位置编码）。  
```
  输入: tensor([
    [0.12, -0.34, 0.56, ..., 0.78],
    [0.45, 0.67, -0.23, ..., 0.19],
    [0.89, -0.12, 0.33, ..., -0.45]
  ])
  位置编码: tensor([
    [0.00, 1.00, 0.00, ..., 0.99],  # 位置 0
    [0.84, 0.54, 0.01, ..., 0.98],  # 位置 1
    [0.91, -0.42, 0.02, ..., 0.97]  # 位置 2
  ])（假设的正弦/余弦值）
  输出: tensor([
    [0.12, 0.66, 0.56, ..., 1.77],
    [1.29, 1.21, -0.22, ..., 1.17],
    [1.80, -0.54, 0.35, ..., 0.52]
  ])（词嵌入 + 位置编码）
```

---

#### 3. 多头自注意力层 (Multi-Head Self-Attention)
**操作**: 使用 8 个注意力头，计算自注意力。  
- 假设 d_model = 512，单头维度 = 512 / 8 = 64。  
- 输入维度: [3, 512]。  
- 输出维度: [3, 512]（注意力加权后的结果）。  
```
  输入: tensor([
    [0.12, 0.66, 0.56, ..., 1.77],
    [1.29, 1.21, -0.22, ..., 1.17],
    [1.80, -0.54, 0.35, ..., 0.52]
  ])
  Q/K/V 计算（假设线性变换后）: 
    Q = [3, 512], K = [3, 512], V = [3, 512]
  注意力分数（简化假设）: tensor([
    [0.7, 0.2, 0.1],  # "这是" 对每个词的注意力
    [0.3, 0.6, 0.1],  # "训练" 对每个词的注意力
    [0.1, 0.2, 0.7]   # "语料" 对每个词的注意力
  ])
  输出: tensor([
    [0.25, 0.45, 0.67, ..., 1.50],
    [0.85, 0.95, -0.15, ..., 1.20],
    [1.60, -0.40, 0.40, ..., 0.60]
  ])（多头注意力加权结果，形状: [3, 512]）
```

---

#### 4. 前馈神经网络层 (Feed-Forward Network)
**操作**: 每个位置独立应用全连接层，ReLU 激活。  
- 内部维度扩展到 2048，再压缩回 512。  
- 输入维度: [3, 512]。  
- 输出维度: [3, 512]。  
```
  输入: tensor([
    [0.25, 0.45, 0.67, ..., 1.50],
    [0.85, 0.95, -0.15, ..., 1.20],
    [1.60, -0.40, 0.40, ..., 0.60]
  ])
  输出: tensor([
    [0.30, 0.50, 0.70, ..., 1.60],
    [0.90, 1.00, 0.00, ..., 1.30],
    [1.70, -0.30, 0.45, ..., 0.65]
  ])（假设前馈网络计算并激活后的结果）
```

---

#### 5. 层归一化 (Layer Normalization)
**操作**: 对每个位置的 512 维向量进行归一化。  
- 输入维度: [3, 512]。  
- 输出维度: [3, 512]。  
```
  输入: tensor([
    [0.30, 0.50, 0.70, ..., 1.60],
    [0.90, 1.00, 0.00, ..., 1.30],
    [1.70, -0.30, 0.45, ..., 0.65]
  ])
  输出: tensor([
    [0.15, 0.25, 0.35, ..., 0.80],
    [0.45, 0.50, 0.00, ..., 0.65],
    [0.85, -0.15, 0.22, ..., 0.32]
  ])（假设归一化后的结果）
```

---

#### 6. 重复编码器层 (Encoder Stack)
**操作**: 假设有 6 个编码器层，重复上述自注意力 + 前馈 + 归一化。  
- 输入维度: [3, 512]。  
- 输出维度: [3, 512]。  
```
  输入: tensor([
    [0.15, 0.25, 0.35, ..., 0.80],
    [0.45, 0.50, 0.00, ..., 0.65],
    [0.85, -0.15, 0.22, ..., 0.32]
  ])
  输出: tensor([
    [0.20, 0.30, 0.40, ..., 0.90],
    [0.55, 0.60, 0.05, ..., 0.70],
    [0.95, -0.10, 0.25, ..., 0.35]
  ])（假设 6 层编码后的结果）
```

---

#### 7. 输出层 (Linear + Softmax)
**操作**: 将最后一个词的表示映射到词汇表大小，计算下一个词的概率。  
- 输入维度: [512]（取最后一个词 "语料" 的表示）。  
- 输出维度: [1000]（词汇表大小）。  
```
  输入: tensor([0.95, -0.10, 0.25, ..., 0.35])（"语料" 的 512 维表示）
  输出（线性变换后）: tensor([2.1, 0.5, -1.2, ..., 1.8])（1000 维 logits）
  Softmax 概率: tensor([0.35, 0.08, 0.01, ..., 0.25])（假设词汇表中词的概率）
  预测词: "的"（假设概率最高）
```

---

### 完整流程总结
1. **输入**: “这是训练语料” → 分词 → ["这是", "训练", "语料"]。
2. **嵌入**: 词嵌入 + 位置编码 → [3, 512]。
3. **编码器**: 多头自注意力 + 前馈网络 + 归一化（重复 6 次） → [3, 512]。
4. **输出**: 取最后一个词表示 → 线性变换 + Softmax → 预测下一个词 "的"。
